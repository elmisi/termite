# Ollama configuration
# Copy this file to .env and modify as needed

# Remote Ollama host (leave empty for localhost)
OLLAMA_HOST=http://localhost:11434

# Models for different phases
REASONING_MODEL=qwen2.5:7b
CODING_MODEL=qwen2.5-coder:14b

# TUI library (rich, textual, urwid, curses)
LIBRARY=rich
